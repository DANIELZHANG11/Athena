services:
  traefik:
    image: zukubq0aouv2k2.xuanyuan.run/traefik:latest
    command:
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --api.insecure=true
    environment:
      # 设置 Docker API 版本以匹配主机
      - DOCKER_API_VERSION=1.44
    ports:
      - "48080:80"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - api
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  api:
    build:
      context: ./api
    env_file:
      - .env
    environment:
      ES_URL: http://opensearch:9200
      REDIS_URL: redis://valkey:6379
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SENTRY_DSN: ""
      DATABASE_URL: postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      CELERY_BROKER_URL: redis://valkey:6379/0
      CELERY_BACKEND_URL: redis://valkey:6379/1
      # JWT 认证密钥 - 必须与 PowerSync 一致
      AUTH_SECRET: ${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      # 认证会话配置 (App-First 模式)
      ACCESS_EXPIRE: ${ACCESS_EXPIRE:-86400}
      REFRESH_EXPIRE: ${REFRESH_EXPIRE:-2592000}
      MINIO_PUBLIC_ENDPOINT: http://localhost:8333
      MINIO_ENDPOINT: seaweed:8333
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: athena
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      SMTP_FROM_EMAIL: ${SMTP_FROM_EMAIL}
      SMTP_USE_SSL: ${SMTP_USE_SSL}
      PAY_FAKE_WEBHOOK_SECRET: devsecret
      # AI 服务配置
      SILICONFLOW_API_KEY: ${SILICONFLOW_API_KEY:-}
      DEFAULT_AI_MODEL: ${DEFAULT_AI_MODEL:-Pro/deepseek-ai/DeepSeek-V3.2}
      AI_API_KEY_ENCRYPTION_KEY: ${AI_API_KEY_ENCRYPTION_KEY:-dev_ai_encryption_key_32bytes!}
      # HuggingFace Cache (共享 worker 下载的模型)
      HF_HOME: /app/.hf_cache
      # 开发模式：跳过 Credits 等校验
      DEV_MODE: "true"
    labels:
      - traefik.enable=true
      - traefik.http.routers.api.rule=Host(`api.youdomin.com`)
      - traefik.http.routers.api.entrypoints=web
      - traefik.http.services.api.loadbalancer.server.port=8000
      - traefik.http.middlewares.api-ratelimit.ratelimit.average=100
      - traefik.http.middlewares.api-ratelimit.ratelimit.burst=50
      - traefik.http.routers.api.middlewares=api-ratelimit
      - traefik.http.routers.api-local.rule=Host(`localhost`)
      - traefik.http.routers.api-local.entrypoints=web
      - traefik.http.routers.api-local.middlewares=api-ratelimit
      - traefik.http.routers.api-local.service=api
    depends_on:
      - pgbouncer
    ports:
      - "48000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./api:/app
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  backup:
    image: zukubq0aouv2k2.xuanyuan.run/postgres:16
    profiles: [ "manual" ]
    entrypoint: [ "bash", "-lc", "pg_dump -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_DB} | gzip > /backups/athena_$(date +%Y%m%d_%H%M).sql.gz" ]
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./backups:/backups
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  postgres:
    image: zukubq0aouv2k2.xuanyuan.run/ankane/pgvector:latest
    environment:
      - POSTGRES_USER=athena
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=athena
      # 启用 replication
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "hba_file=/etc/postgresql/pg_hba.conf"
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "athena" ]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  valkey:
    image: zukubq0aouv2k2.xuanyuan.run/redis:7
    command: [ "redis-server", "--appendonly", "yes" ]
    volumes:
      - valkey_data:/data
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  seaweed:
    image: zukubq0aouv2k2.xuanyuan.run/chrislusf/seaweedfs:latest
    command: [ "server", "-s3", "-s3.port=8333", "-s3.allowedOrigins=*", "-dir=/data" ]
    ports:
      - "48333:8333"
      - "48888:8888"
    volumes:
      - seaweed_data:/data
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  opensearch:
    # 使用自定义构建以包含 IK/Pinyin/STConvert 插件
    build:
      context: ./docker/opensearch
    image: athena-opensearch:custom
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "59200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200/_cluster/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  pgbouncer:
    image: zukubq0aouv2k2.xuanyuan.run/edoburu/pgbouncer:latest
    environment:
      # edoburu/pgbouncer 配置
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=athena
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_NAME=athena
      - POOL_MODE=session
      - MAX_CLIENT_CONN=200
      - DEFAULT_POOL_SIZE=20
      - LISTEN_PORT=6432
      - AUTH_TYPE=md5
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - athena-network
    healthcheck:
      test: [ "CMD", "/bin/sh", "-c", "pg_isready -h localhost -p 6432 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # MongoDB (PowerSync 存储后端)
  # -------------------------------------------------------------------------
  mongo:
    image: zukubq0aouv2k2.xuanyuan.run/mongo:7
    command: [ "--replSet", "rs0", "--bind_ip_all" ]
    ports:
      - "47017:27017"
    volumes:
      - mongo_data:/data/db
      - mongo_configdb:/data/configdb
    healthcheck:
      test: [ "CMD", "mongosh", "--eval", "rs.status().ok || rs.initiate({_id:'rs0',members:[{_id:0,host:'mongo:27017'}]}).ok" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # PowerSync Service (App-First 同步引擎)
  # @see 09 - APP-FIRST架构改造计划.md - Phase 1
  # -------------------------------------------------------------------------
  powersync:
    image: zukubq0aouv2k2.xuanyuan.run/journeyapps/powersync-service:latest
    ports:
      - "${POWERSYNC_PORT:-48080}:8080"
      - "49091:9090" # Prometheus metrics
    environment:
      # PowerSync 要求 PS_ 前缀的环境变量
      - PS_DATABASE_URI=postgresql://athena:${POSTGRES_PASSWORD}@postgres:5432/athena
      - PS_SUPABASE_JWT_SECRET=${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      - PS_MONGO_URI=mongodb://mongo:27017/powersync?replicaSet=rs0
      - PS_LOG_LEVEL=${POWERSYNC_LOG_LEVEL:-info}
      # 兼容性保留
      - POWERSYNC_JWT_SECRET=${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      - POWERSYNC_UPLOAD_ENABLED=${POWERSYNC_UPLOAD_ENABLED:-true}
      - POWERSYNC_LOG_LEVEL=${POWERSYNC_LOG_LEVEL:-info}
    volumes:
      - ./docker/powersync/powersync.yaml:/app/powersync.yaml:ro
      - ./docker/powersync/sync_rules.yaml:/app/sync_rules.yaml:ro
    depends_on:
      postgres:
        condition: service_healthy
      mongo:
        condition: service_healthy
    healthcheck:
      # PowerSync 正确的健康检查端点是 /probes/liveness
      test: [ "CMD", "node", "-e", "require('http').get('http://localhost:8080/probes/liveness', r => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  calibre:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_config:/config
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    ports:
      - "48081:8080"
      - "48082:8081"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  # Calibre 转换监控器 - 监听转换请求并执行 ebook-convert
  calibre-watcher:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_config:/config
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    # 启动时运行 watcher 脚本
    entrypoint: [ "/bin/bash", "/scripts/convert-watcher.sh" ]
    command: []
    depends_on:
      - calibre
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "2"

  # Calibre 元数据提取监控器 - 使用 ebook-meta 即时提取非 EPUB/PDF 的元数据和封面
  calibre-metadata:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_config:/config
      - calibre_books:/books
      - ./scripts/calibre-metadata-watcher.sh:/scripts/metadata-watcher.sh:ro
    entrypoint: [ "/bin/bash", "/scripts/metadata-watcher.sh" ]
    command: []
    depends_on:
      - calibre
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "2"

  # =========================================================================
  # Worker 配置 - GPU/CPU 分离策略
  # =========================================================================
  # 队列优先级:
  # - gpu_high: OCR（付费服务）
  # - gpu_low: 向量索引（免费服务）
  # - cpu_default: 元数据提取、封面提取等
  # =========================================================================

  # GPU Worker - 专用处理 OCR 和向量索引（串行避免显存竞争）
  worker-gpu:
    build:
      context: ./api
      args:
        SKIP_HEAVY: "false" # 安装 PaddleOCR + BGE-M3 + EdgeTTS
    environment:
      - CELERY_BROKER_URL=redis://valkey:6379/0
      - CELERY_BACKEND_URL=redis://valkey:6379/1
      - REDIS_URL=redis://valkey:6379
      - DATABASE_URL=postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      - MINIO_ENDPOINT=seaweed:8333
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=athena
      - ES_URL=http://opensearch:9200
      - CALIBRE_CONVERT_DIR=/calibre_books
      # OCR 配置 (PP-OCRv5 mobile)
      - OCR_USE_GPU=true
      - OCR_GPU_MEM=4000
      - OCR_CPU_THREADS=8
      - OCR_LANG=ch
      # Embedding 配置 (BGE-M3) - 内存优化
      - EMBEDDING_MODEL_NAME=BAAI/bge-m3
      - EMBEDDING_USE_GPU=true
      - EMBEDDING_BATCH_SIZE=16 # 从 32 降低到 16
      - INDEX_NODE_BATCH_SIZE=450 # 450 节点 ≈ 20万字/本
      - GPU_MIN_FREE_GB=2.0 # 显存阈值，可调整
      - HF_HOME=/app/.hf_cache
      # GPU 内存控制 (RTX 3060 12GB / RTX 3070 8GB)
      - FLAGS_fraction_of_gpu_memory_to_use=0.5
      # 禁用 NCCL (解决 ncclGroupSimulateEnd symbol 问题)
      - NCCL_P2P_DISABLE=1
      - NCCL_IB_DISABLE=1
      - TORCH_NCCL_ASYNC_ERROR_HANDLING=0
      # 【2026-01-09】模型预加载所需
      - CELERY_QUEUES=gpu_high,gpu_low
      # 【2026-01-09】离线模式 - 暂时禁用，因为模型尚未预下载到 Docker 镜像
      # 模型首次下载后可以启用这些环境变量：
      # - HF_HUB_OFFLINE=1
      # - TRANSFORMERS_OFFLINE=1
      # - HF_DATASETS_OFFLINE=1
      # 并发=1：GPU 任务串行执行，避免显存竞争
      # 监听 gpu_high (OCR) 和 gpu_low (向量索引) 队列
    command: [ "celery", "-A", "app.celery_app.celery_app", "worker", "-Q", "gpu_high,gpu_low", "-l", "INFO", "--concurrency=1", "--pool=prefork", "--max-tasks-per-child=50" ]
    depends_on:
      - valkey
      - pgbouncer
      - seaweed
      - opensearch
    volumes:
      - ./api:/app
      - calibre_books:/calibre_books
      - hf_cache:/app/.hf_cache
    # GPU 支持（需要 nvidia-docker）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # CPU Worker - 处理非 GPU 任务（元数据、封面、转换等）
  worker-cpu:
    build:
      context: ./api
      args:
        SKIP_HEAVY: "true" # 不需要 PaddleOCR/BGE-M3
    environment:
      - CELERY_BROKER_URL=redis://valkey:6379/0
      - CELERY_BACKEND_URL=redis://valkey:6379/1
      - REDIS_URL=redis://valkey:6379
      - DATABASE_URL=postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      - MINIO_ENDPOINT=seaweed:8333
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=athena
      - ES_URL=http://opensearch:9200
      - CALIBRE_CONVERT_DIR=/calibre_books
    # 并发=4：CPU 任务可并行
    # 监听 cpu_default 队列
    command: [ "celery", "-A", "app.celery_app.celery_app", "worker", "-Q", "cpu_default", "-l", "INFO", "--concurrency=4", "--pool=prefork", "--max-tasks-per-child=200" ]
    depends_on:
      - valkey
      - pgbouncer
      - seaweed
    volumes:
      - ./api:/app
      - calibre_books:/calibre_books
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

# =========================================================================
# 数据卷配置 - SSD + HDD 混合存储方案
# =========================================================================
# 存储策略说明：
# 1. 高性能需求（随机读写频繁）-> SSD (/home/vitiana/Athena/data_ssd/)
#    - PostgreSQL: 数据库事务日志，IOPS 敏感
#    - OpenSearch: 全文索引，查询性能要求高
#    - Valkey (Redis): AOF 持久化，写入频繁
#    - HuggingFace Cache: 模型加载时间敏感
#
# 2. 大容量需求（顺序读写为主）-> bcache HDD (/data/athena/)
#    - SeaweedFS: 对象存储，大文件存储
#    - Calibre: 电子书库，大文件存储
#    - Tolgee: 低频访问的翻译数据库
#
# 性能测试结果：
#   - SSD 顺序写: 932 MB/s, 随机读 4K: 11,200 IOPS
#   - bcache 顺序写: 575 MB/s (writearound 模式)
# =========================================================================

volumes:
  # Docker 默认卷 - 跨平台兼容
  # 生产环境可通过 docker-compose.override.yml 覆盖为绑定路径
  pg_data:
  valkey_data:
  opensearch_data:
  hf_cache:
  seaweed_data:
  calibre_books:
  calibre_config:
  mongo_data:
  mongo_configdb:


networks:
  athena-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
