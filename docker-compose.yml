services:
  traefik:
    image: zukubq0aouv2k2.xuanyuan.run/traefik:v3.0
    command:
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --api.insecure=true
    ports:
      - "48080:80"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - api
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  api:
    build:
      context: ./api
    env_file:
      - .env
      - .env.local
      - .env.infisical
    environment:
      ES_URL: http://opensearch:9200
      REDIS_URL: redis://valkey:6379
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SENTRY_DSN: ""
      DATABASE_URL: postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      CELERY_BROKER_URL: redis://valkey:6379/0
      CELERY_BACKEND_URL: redis://valkey:6379/1
      MINIO_PUBLIC_ENDPOINT: http://localhost:8333
      MINIO_ENDPOINT: seaweed:8333
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: athena
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      SMTP_FROM_EMAIL: ${SMTP_FROM_EMAIL}
      SMTP_USE_SSL: ${SMTP_USE_SSL}
      PAY_FAKE_WEBHOOK_SECRET: devsecret
    labels:
      - traefik.enable=true
      - traefik.http.routers.api.rule=Host(`api.youdomin.com`)
      - traefik.http.routers.api.entrypoints=web
      - traefik.http.services.api.loadbalancer.server.port=8000
      - traefik.http.middlewares.api-ratelimit.ratelimit.average=100
      - traefik.http.middlewares.api-ratelimit.ratelimit.burst=50
      - traefik.http.routers.api.middlewares=api-ratelimit
      - traefik.http.routers.api-local.rule=Host(`localhost`)
      - traefik.http.routers.api-local.entrypoints=web
      - traefik.http.routers.api-local.middlewares=api-ratelimit
      - traefik.http.routers.api-local.service=api
    depends_on:
      - pgbouncer
    ports:
      - "48000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./api:/app
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  backup:
    image: zukubq0aouv2k2.xuanyuan.run/postgres:16
    profiles: [ "manual" ]
    entrypoint: [ "bash", "-lc", "pg_dump -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_DB} | gzip > /backups/athena_$(date +%Y%m%d_%H%M).sql.gz" ]
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./backups:/backups
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  postgres:
    image: zukubq0aouv2k2.xuanyuan.run/ankane/pgvector:latest
    environment:
      - POSTGRES_USER=athena
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=athena
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "athena" ]
      interval: 10s
      timeout: 5s
      retries: 10
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  valkey:
    image: zukubq0aouv2k2.xuanyuan.run/redis:7
    command: [ "redis-server", "--appendonly", "yes" ]
    volumes:
      - valkey_data:/data
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  seaweed:
    image: zukubq0aouv2k2.xuanyuan.run/chrislusf/seaweedfs:latest
    command: [ "server", "-s3", "-s3.port=8333", "-s3.allowedOrigins=*", "-dir=/data" ]
    ports:
      - "48333:8333"
      - "48888:8888"
    volumes:
      - seaweed_data:/data
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  opensearch:
    # 使用自定义构建以包含 IK/Pinyin/STConvert 插件
    build:
      context: ./docker/opensearch
    image: athena-opensearch:custom
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "49200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200/_cluster/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  pgbouncer:
    image: zukubq0aouv2k2.xuanyuan.run/brainsam/pgbouncer:latest
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=athena
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_NAME=athena
      - POOL_MODE=session
      - MAX_CLIENT_CONN=200
      - DEFAULT_POOL_SIZE=20
      - LISTEN_PORT=6432
    depends_on:
      - postgres
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # PowerSync Service (App-First 同步引擎)
  # @see 09 - APP-FIRST架构改造计划.md - Phase 1
  # -------------------------------------------------------------------------
  powersync:
    image: zukubq0aouv2k2.xuanyuan.run/journeyapps/powersync-service:latest
    ports:
      - "${POWERSYNC_PORT:-48090}:8090"
      - "49091:9090"  # Prometheus metrics
    environment:
      # 数据库连接（直连 PostgreSQL，不经过 PgBouncer）
      - POWERSYNC_DATABASE_URL=postgresql://athena:${POSTGRES_PASSWORD}@postgres:5432/athena
      # JWT 认证密钥
      - POWERSYNC_JWT_SECRET=${POWERSYNC_JWT_SECRET:-dev_powersync_secret}
      # 上传控制
      - POWERSYNC_UPLOAD_ENABLED=${POWERSYNC_UPLOAD_ENABLED:-true}
      # 日志级别
      - POWERSYNC_LOG_LEVEL=${POWERSYNC_LOG_LEVEL:-info}
    volumes:
      - ./docker/powersync/powersync.yaml:/config/powersync.yaml:ro
      - ./docker/powersync/sync_rules.yaml:/config/sync_rules.yaml:ro
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # Tolgee (Translation Platform)
  # -------------------------------------------------------------------------
  tolgee_db:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: tolgee
      POSTGRES_PASSWORD: tolgee_password
      POSTGRES_DB: tolgee
    volumes:
      - tolgee_postgres_data:/var/lib/postgresql/data
    networks:
      - athena-network

  tolgee:
    image: tolgee/tolgee:latest
    ports:
      - "48085:8080"
    environment:
      TOLGEE_DB_URL: jdbc:postgresql://tolgee_db:5432/tolgee
      TOLGEE_DB_USER: tolgee
      TOLGEE_DB_PASSWORD: tolgee_password
      TOLGEE_AUTHENTICATION_ENABLED: true
      TOLGEE_AUTHENTICATION_INITIAL_USERNAME: admin
      TOLGEE_AUTHENTICATION_INITIAL_PASSWORD: admin
    depends_on:
      - tolgee_db
    networks:
      - athena-network

  calibre:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_config:/config
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    ports:
      - "48081:8080"
      - "48082:8081"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  # Calibre 转换监控器 - 监听转换请求并执行 ebook-convert
  calibre-watcher:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    # 启动时运行 watcher 脚本
    entrypoint: ["/bin/bash", "/scripts/convert-watcher.sh"]
    command: []
    depends_on:
      - calibre
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "2"
  worker:
    build:
      context: ./api
      args:
        SKIP_HEAVY: "false"  # 安装 PaddleOCR + BGE-M3 + EdgeTTS
    environment:
      - CELERY_BROKER_URL=redis://valkey:6379/0
      - CELERY_BACKEND_URL=redis://valkey:6379/1
      - DATABASE_URL=postgresql+asyncpg://athena:athena@pgbouncer:6432/athena
      - MINIO_ENDPOINT=seaweed:8333
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=athena
      - CALIBRE_CONVERT_DIR=/calibre_books
      # OCR 配置 (PP-OCRv5 mobile)
      - OCR_USE_GPU=true
      - OCR_GPU_MEM=3500
      - OCR_CPU_THREADS=6
      - OCR_LANG=ch
      # Embedding 配置 (BGE-M3)
      - EMBEDDING_MODEL_NAME=BAAI/bge-m3
      - HF_HOME=/app/.hf_cache
      # GPU 内存控制
      - FLAGS_fraction_of_gpu_memory_to_use=0.4
    # 2 Workers（开发环境 8GB GPU）
    # 生产环境 12GB GPU 可改为 --concurrency=3
    command: ["celery", "-A", "app.celery_app.celery_app", "worker", "-l", "INFO", "--concurrency=2", "--pool=prefork", "--max-tasks-per-child=50"]
    depends_on:
      - valkey
      - pgbouncer
      - seaweed
      - opensearch
    volumes:
      - ./api:/app
      - calibre_books:/calibre_books
      - hf_cache:/app/.hf_cache
    # GPU 支持（需要 nvidia-docker）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
# =========================================================================
# 数据卷配置 - SSD + HDD 混合存储方案
# =========================================================================
# 存储策略说明：
# 1. 高性能需求（随机读写频繁）-> SSD (/home/vitiana/Athena/data_ssd/)
#    - PostgreSQL: 数据库事务日志，IOPS 敏感
#    - OpenSearch: 全文索引，查询性能要求高
#    - Valkey (Redis): AOF 持久化，写入频繁
#    - HuggingFace Cache: 模型加载时间敏感
#
# 2. 大容量需求（顺序读写为主）-> bcache HDD (/data/athena/)
#    - SeaweedFS: 对象存储，大文件存储
#    - Calibre: 电子书库，大文件存储
#    - Tolgee: 低频访问的翻译数据库
#
# 性能测试结果：
#   - SSD 顺序写: 932 MB/s, 随机读 4K: 11,200 IOPS
#   - bcache 顺序写: 575 MB/s (writearound 模式)
# =========================================================================

volumes:
  # --- 高性能存储（SSD）---
  pg_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/vitiana/Athena/data_ssd/postgres
  
  valkey_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/vitiana/Athena/data_ssd/valkey
  
  opensearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/vitiana/Athena/data_ssd/opensearch
  
  hf_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /home/vitiana/Athena/data_ssd/hf_cache
  
  # --- 大容量存储（bcache HDD）---
  seaweed_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/athena/seaweed
  
  calibre_books:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/athena/calibre_books
  
  calibre_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/athena/calibre_config
  
  tolgee_postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/athena/tolgee


networks:
  athena-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
