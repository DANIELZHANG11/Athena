services:
  traefik:
    image: zukubq0aouv2k2.xuanyuan.run/traefik:latest
    command:
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --api.insecure=true
    environment:
      # 设置 Docker API 版本以匹配主机
      - DOCKER_API_VERSION=1.44
    ports:
      - "48080:80"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - api
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  api:
    build:
      context: ./api
    env_file:
      - .env
    environment:
      ES_URL: http://opensearch:9200
      REDIS_URL: redis://valkey:6379
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SENTRY_DSN: ""
      DATABASE_URL: postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      CELERY_BROKER_URL: redis://valkey:6379/0
      CELERY_BACKEND_URL: redis://valkey:6379/1
      # JWT 认证密钥 - 必须与 PowerSync 一致
      AUTH_SECRET: ${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      # 认证会话配置 (App-First 模式)
      ACCESS_EXPIRE: ${ACCESS_EXPIRE:-86400}
      REFRESH_EXPIRE: ${REFRESH_EXPIRE:-2592000}
      MINIO_PUBLIC_ENDPOINT: http://localhost:8333
      MINIO_ENDPOINT: seaweed:8333
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      MINIO_BUCKET: athena
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      SMTP_FROM_EMAIL: ${SMTP_FROM_EMAIL}
      SMTP_USE_SSL: ${SMTP_USE_SSL}
      PAY_FAKE_WEBHOOK_SECRET: devsecret
    labels:
      - traefik.enable=true
      - traefik.http.routers.api.rule=Host(`api.youdomin.com`)
      - traefik.http.routers.api.entrypoints=web
      - traefik.http.services.api.loadbalancer.server.port=8000
      - traefik.http.middlewares.api-ratelimit.ratelimit.average=100
      - traefik.http.middlewares.api-ratelimit.ratelimit.burst=50
      - traefik.http.routers.api.middlewares=api-ratelimit
      - traefik.http.routers.api-local.rule=Host(`localhost`)
      - traefik.http.routers.api-local.entrypoints=web
      - traefik.http.routers.api-local.middlewares=api-ratelimit
      - traefik.http.routers.api-local.service=api
    depends_on:
      - pgbouncer
    ports:
      - "48000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./api:/app
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  backup:
    image: zukubq0aouv2k2.xuanyuan.run/postgres:16
    profiles: [ "manual" ]
    entrypoint: [ "bash", "-lc", "pg_dump -h postgres -U ${POSTGRES_USER} -d ${POSTGRES_DB} | gzip > /backups/athena_$(date +%Y%m%d_%H%M).sql.gz" ]
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ./backups:/backups
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  postgres:
    image: zukubq0aouv2k2.xuanyuan.run/ankane/pgvector:latest
    environment:
      - POSTGRES_USER=athena
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=athena
      # 启用 replication
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_replication_slots=10"
      - "-c"
      - "max_wal_senders=10"
      - "-c"
      - "hba_file=/etc/postgresql/pg_hba.conf"
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "athena" ]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  valkey:
    image: zukubq0aouv2k2.xuanyuan.run/redis:7
    command: [ "redis-server", "--appendonly", "yes" ]
    volumes:
      - valkey_data:/data
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  seaweed:
    image: zukubq0aouv2k2.xuanyuan.run/chrislusf/seaweedfs:latest
    command: [ "server", "-s3", "-s3.port=8333", "-s3.allowedOrigins=*", "-dir=/data" ]
    ports:
      - "48333:8333"
      - "48888:8888"
    volumes:
      - seaweed_data:/data
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  opensearch:
    # 使用自定义构建以包含 IK/Pinyin/STConvert 插件
    build:
      context: ./docker/opensearch
    image: athena-opensearch:custom
    environment:
      - discovery.type=single-node
      - DISABLE_SECURITY_PLUGIN=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "59200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200/_cluster/health" ]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  pgbouncer:
    image: zukubq0aouv2k2.xuanyuan.run/edoburu/pgbouncer:latest
    environment:
      # edoburu/pgbouncer 配置
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=athena
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_NAME=athena
      - POOL_MODE=session
      - MAX_CLIENT_CONN=200
      - DEFAULT_POOL_SIZE=20
      - LISTEN_PORT=6432
      - AUTH_TYPE=md5
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - athena-network
    healthcheck:
      test: [ "CMD", "/bin/sh", "-c", "pg_isready -h localhost -p 6432 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # MongoDB (PowerSync 存储后端)
  # -------------------------------------------------------------------------
  mongo:
    image: zukubq0aouv2k2.xuanyuan.run/mongo:7
    command: [ "--replSet", "rs0", "--bind_ip_all" ]
    ports:
      - "47017:27017"
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: [ "CMD", "mongosh", "--eval", "rs.status().ok || rs.initiate({_id:'rs0',members:[{_id:0,host:'mongo:27017'}]}).ok" ]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # -------------------------------------------------------------------------
  # PowerSync Service (App-First 同步引擎)
  # @see 09 - APP-FIRST架构改造计划.md - Phase 1
  # -------------------------------------------------------------------------
  powersync:
    image: zukubq0aouv2k2.xuanyuan.run/journeyapps/powersync-service:latest
    ports:
      - "${POWERSYNC_PORT:-48080}:8080"
      - "49091:9090" # Prometheus metrics
    environment:
      # PowerSync 要求 PS_ 前缀的环境变量
      - PS_DATABASE_URI=postgresql://athena:${POSTGRES_PASSWORD}@postgres:5432/athena
      - PS_SUPABASE_JWT_SECRET=${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      - PS_MONGO_URI=mongodb://mongo:27017/powersync?replicaSet=rs0
      - PS_LOG_LEVEL=${POWERSYNC_LOG_LEVEL:-info}
      # 兼容性保留
      - POWERSYNC_JWT_SECRET=${AUTH_SECRET:-dev_powersync_secret_change_in_production}
      - POWERSYNC_UPLOAD_ENABLED=${POWERSYNC_UPLOAD_ENABLED:-true}
      - POWERSYNC_LOG_LEVEL=${POWERSYNC_LOG_LEVEL:-info}
    volumes:
      - ./docker/powersync/powersync.yaml:/app/powersync.yaml:ro
      - ./docker/powersync/sync_rules.yaml:/app/sync_rules.yaml:ro
    depends_on:
      postgres:
        condition: service_healthy
      mongo:
        condition: service_healthy
    healthcheck:
      # PowerSync 正确的健康检查端点是 /probes/liveness
      test: [ "CMD", "node", "-e", "require('http').get('http://localhost:8080/probes/liveness', r => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  calibre:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_config:/config
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    ports:
      - "48081:8080"
      - "48082:8081"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
  # Calibre 转换监控器 - 监听转换请求并执行 ebook-convert
  calibre-watcher:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_books:/books
      - ./scripts/calibre-convert-watcher.sh:/scripts/convert-watcher.sh:ro
    # 启动时运行 watcher 脚本
    entrypoint: [ "/bin/bash", "/scripts/convert-watcher.sh" ]
    command: []
    depends_on:
      - calibre
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "2"

  # Calibre 元数据提取监控器 - 使用 ebook-meta 即时提取非 EPUB/PDF 的元数据和封面
  calibre-metadata:
    image: zukubq0aouv2k2.xuanyuan.run/linuxserver/calibre:latest
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Shanghai
    volumes:
      - calibre_books:/books
      - ./scripts/calibre-metadata-watcher.sh:/scripts/metadata-watcher.sh:ro
    entrypoint: [ "/bin/bash", "/scripts/metadata-watcher.sh" ]
    command: []
    depends_on:
      - calibre
    restart: unless-stopped
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "2"

  worker:
    build:
      context: ./api
      args:
        SKIP_HEAVY: "false" # 安装 PaddleOCR + BGE-M3 + EdgeTTS
    environment:
      - CELERY_BROKER_URL=redis://valkey:6379/0
      - CELERY_BACKEND_URL=redis://valkey:6379/1
      - DATABASE_URL=postgresql+asyncpg://athena:${POSTGRES_PASSWORD}@pgbouncer:6432/athena
      - MINIO_ENDPOINT=seaweed:8333
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=athena
      - CALIBRE_CONVERT_DIR=/calibre_books
      # OCR 配置 (PP-OCRv5 mobile)
      - OCR_USE_GPU=true
      - OCR_GPU_MEM=4000
      - OCR_CPU_THREADS=8
      - OCR_LANG=ch
      # Embedding 配置 (BGE-M3)
      - EMBEDDING_MODEL_NAME=BAAI/bge-m3
      - HF_HOME=/app/.hf_cache
      # GPU 内存控制 (RTX 3060 12GB: PaddleOCR ~3GB + BGE-M3 ~3GB)
      - FLAGS_fraction_of_gpu_memory_to_use=0.5
    # 3 Workers (RTX 3060 12GB GPU，最佳并发)
    # PaddleOCR + BGE-M3 约占用 6-7GB，剩余约 5GB 用于推理缓冲
    command: [ "celery", "-A", "app.celery_app.celery_app", "worker", "-l", "INFO", "--concurrency=3", "--pool=prefork", "--max-tasks-per-child=100" ]
    depends_on:
      - valkey
      - pgbouncer
      - seaweed
      - opensearch
    volumes:
      - ./api:/app
      - calibre_books:/calibre_books
      - hf_cache:/app/.hf_cache
    # GPU 支持（需要 nvidia-docker）
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - athena-network
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"
# =========================================================================
# 数据卷配置 - SSD + HDD 混合存储方案
# =========================================================================
# 存储策略说明：
# 1. 高性能需求（随机读写频繁）-> SSD (/home/vitiana/Athena/data_ssd/)
#    - PostgreSQL: 数据库事务日志，IOPS 敏感
#    - OpenSearch: 全文索引，查询性能要求高
#    - Valkey (Redis): AOF 持久化，写入频繁
#    - HuggingFace Cache: 模型加载时间敏感
#
# 2. 大容量需求（顺序读写为主）-> bcache HDD (/data/athena/)
#    - SeaweedFS: 对象存储，大文件存储
#    - Calibre: 电子书库，大文件存储
#    - Tolgee: 低频访问的翻译数据库
#
# 性能测试结果：
#   - SSD 顺序写: 932 MB/s, 随机读 4K: 11,200 IOPS
#   - bcache 顺序写: 575 MB/s (writearound 模式)
# =========================================================================

volumes:
  # Docker 默认卷 - 跨平台兼容
  # 生产环境可通过 docker-compose.override.yml 覆盖为绑定路径
  pg_data:
  valkey_data:
  opensearch_data:
  hf_cache:
  seaweed_data:
  calibre_books:
  calibre_config:
  mongo_data:


networks:
  athena-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
