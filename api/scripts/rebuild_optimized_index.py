"""
å‘é‡ç´¢å¼•ä¼˜åŒ–é‡å»ºè„šæœ¬

æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. åˆ é™¤ç°æœ‰çš„ athena_book_chunks ç´¢å¼•
2. ä½¿ç”¨ä¼˜åŒ–åçš„ mapping é‡æ–°åˆ›å»ºç´¢å¼•
3. è§¦å‘æ‰€æœ‰å·²ä¸Šä¼ ä¹¦ç±çš„é‡æ–°ç´¢å¼•ä»»åŠ¡

ä¼˜åŒ–å†…å®¹ï¼š
- EPUBç« èŠ‚æå–ï¼šåŸºäº toc.ncx/nav.xhtml æ ‡å‡†è§£æï¼ˆéæ­£åˆ™çŒœæµ‹ï¼‰
- å‘é‡å­˜å‚¨ï¼šç§»é™¤ LlamaIndex å†—ä½™å­—æ®µï¼ˆ_node_content, original_text ç­‰ï¼‰
- å‘é‡ç²¾åº¦ï¼šfloat16 é‡åŒ–ï¼ˆèŠ‚çœ 30% å­˜å‚¨ç©ºé—´ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
docker exec athena-api-1 python scripts/rebuild_optimized_index.py
"""

import asyncio
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from opensearchpy import AsyncOpenSearch
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine

from app.services.llama_rag import (
    OPENSEARCH_URL,
    BOOK_CHUNKS_INDEX,
    recreate_book_chunks_index,
)

DATABASE_URL = os.getenv(
    "DATABASE_URL", "postgresql+asyncpg://athena:athena_dev@postgres:5432/athena"
)


async def get_index_stats():
    """è·å–å½“å‰ç´¢å¼•ç»Ÿè®¡"""
    client = AsyncOpenSearch(hosts=[OPENSEARCH_URL])
    try:
        exists = await client.indices.exists(index=BOOK_CHUNKS_INDEX)
        if not exists:
            return {"exists": False, "count": 0, "size_mb": 0}
        
        stats = await client.indices.stats(index=BOOK_CHUNKS_INDEX)
        index_stats = stats['indices'][BOOK_CHUNKS_INDEX]['primaries']
        
        return {
            "exists": True,
            "count": index_stats['docs']['count'],
            "size_mb": round(index_stats['store']['size_in_bytes'] / 1024 / 1024, 2),
        }
    finally:
        await client.close()


async def get_all_books():
    """è·å–æ•°æ®åº“ä¸­æ‰€æœ‰éœ€è¦ç´¢å¼•çš„ä¹¦ç±"""
    engine = create_async_engine(DATABASE_URL)
    
    async with engine.connect() as conn:
        # è·å–æ‰€æœ‰ä¸Šä¼ å®Œæˆçš„ä¹¦ç±
        result = await conn.execute(text("""
            SELECT id, title, file_path, file_type, upload_status 
            FROM books 
            WHERE upload_status IN ('completed', 'text_extracted')
            ORDER BY created_at DESC
        """))
        books = result.fetchall()
    
    await engine.dispose()
    return books


async def queue_index_tasks(books):
    """ä¸ºæ‰€æœ‰ä¹¦ç±æ’é˜Ÿç´¢å¼•ä»»åŠ¡"""
    from app.tasks.index_tasks import create_book_vector_index
    
    queued = 0
    for book in books:
        book_id = str(book.id)
        title = book.title
        
        print(f"  æ’é˜Ÿ: {title} ({book_id})")
        create_book_vector_index.delay(book_id)
        queued += 1
    
    return queued


async def main():
    print("=" * 60)
    print("å‘é‡ç´¢å¼•ä¼˜åŒ–é‡å»ºè„šæœ¬")
    print("=" * 60)
    print()
    
    # 1. æ˜¾ç¤ºå½“å‰çŠ¶æ€
    print("ğŸ“Š å½“å‰ç´¢å¼•çŠ¶æ€:")
    stats = await get_index_stats()
    if stats["exists"]:
        print(f"   - æ–‡æ¡£æ•°: {stats['count']}")
        print(f"   - å¤§å°: {stats['size_mb']} MB")
    else:
        print("   - ç´¢å¼•ä¸å­˜åœ¨")
    print()
    
    # 2. è·å–æ‰€æœ‰ä¹¦ç±
    print("ğŸ“š è·å–æ•°æ®åº“ä¸­çš„ä¹¦ç±...")
    books = await get_all_books()
    print(f"   æ‰¾åˆ° {len(books)} æœ¬ä¹¦ç±éœ€è¦ç´¢å¼•")
    print()
    
    # 3. ç¡®è®¤æ“ä½œ
    print("âš ï¸  è­¦å‘Š: è¿™å°†åˆ é™¤æ‰€æœ‰ç°æœ‰å‘é‡ç´¢å¼•å¹¶é‡å»ºï¼")
    confirm = input("ç¡®è®¤ç»§ç»­? (è¾“å…¥ 'yes' ç¡®è®¤): ")
    if confirm.lower() != 'yes':
        print("å·²å–æ¶ˆ")
        return
    
    print()
    
    # 4. é‡å»ºç´¢å¼•
    print("ğŸ”„ åˆ é™¤å¹¶é‡å»ºç´¢å¼•...")
    result = await recreate_book_chunks_index()
    print(f"   ç»“æœ: {result}")
    print()
    
    # 5. æ’é˜Ÿç´¢å¼•ä»»åŠ¡
    print("ğŸ“¤ æ’é˜Ÿä¹¦ç±ç´¢å¼•ä»»åŠ¡...")
    queued = await queue_index_tasks(books)
    print(f"   å·²æ’é˜Ÿ {queued} ä¸ªä»»åŠ¡")
    print()
    
    print("âœ… å®Œæˆï¼è¯·æŸ¥çœ‹ Celery worker æ—¥å¿—æŸ¥çœ‹ç´¢å¼•è¿›åº¦")
    print("   ç›‘æ§å‘½ä»¤: docker logs -f athena-worker-gpu-1")


if __name__ == "__main__":
    asyncio.run(main())
